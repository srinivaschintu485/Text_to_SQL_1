{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "O5Ke6n2HoGW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ],
      "metadata": {
        "id": "XT5r8CVXoJU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install faiss-cpu\n",
        "import faiss"
      ],
      "metadata": {
        "id": "gGDqt1CmfUl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# converting to embeddings"
      ],
      "metadata": {
        "id": "ffpnWPSIoNGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema_chunks = [\n",
        "     \"Table: customers\\nColumns: customer_id (INT), name (TEXT), email (TEXT), region (TEXT)\",\n",
        "    \"Table: orders\\nColumns: order_id (INT), customer_id (INT), order_date (DATE), total_amount (FLOAT)\",\n",
        "    \"Table: order_items\\nColumns: order_item_id (INT), order_id (INT), product_id (INT), quantity (INT), item_total (FLOAT)\",\n",
        "    \"Table: products\\nColumns: product_id (INT), product_name (TEXT), category (TEXT), price (FLOAT)\",\n",
        "    \"Table: reviews\\nColumns: review_id (INT), product_id (INT), customer_id (INT), rating (INT), comment (TEXT)\"\n",
        "]\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "schema_vectors = embed_model.encode(schema_chunks)\n",
        "index = faiss.IndexFlatL2(schema_vectors.shape[1]) #he number inside IndexFlatL2(...) is the dimension of each vector, which is usually something like 384 or 768\n",
        "index.add(schema_vectors)"
      ],
      "metadata": {
        "id": "WbXrh3zHcGkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, text in enumerate(schema_chunks):\n",
        "    print(f\"Index {i}:\\nSchema Text:\\n{text}\\nVector (first 10 dimensions): {schema_vectors[i][:10]}\\n{'-'*80}\")"
      ],
      "metadata": {
        "id": "GgXpMiHFcGns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q-DZ_K-wcGq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM we are using are defog/sqlcoder-7b-2"
      ],
      "metadata": {
        "id": "FZykuO-LoUtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_id = \"defog/sqlcoder-7b-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "IPbOpElPcG5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sZNsc4hKcrul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query"
      ],
      "metadata": {
        "id": "gs8s6jXBodcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_bank = {\n",
        "    \"very_easy\": [\n",
        "        \"how many customers have same first name?\",\n",
        "        \"Count the number of customers in each region.\"\n",
        "    ],\n",
        "    \"easy\": [\n",
        "        \"Show total revenue per region in 2024.\",\n",
        "        \"What is the average rating of each product?\"\n",
        "    ],\n",
        "    \"hard\": [\n",
        "        \"Which products had more than 100 units sold in total in 2024?\",\n",
        "        \"Show top 5 customers by spending in each region.\"\n",
        "    ],\n",
        "    \"tough\": [\n",
        "        \"Find all orders where the average item price is above $100.\",\n",
        "        \"List all customers who made purchases in more than 3 different months of 2024.\"\n",
        "    ],\n",
        "    \"toughest\": [\n",
        "        \"For each product category, identify the top 3 regions with the highest total revenue generated in 2024. For each region, show the average order value, the total number of distinct customers, and the average product rating for that category. Only include regions where at least 5 unique customers have placed orders for products in that category.\"\n",
        "    ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "CO1YdwOQcrx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3OA0n8ewcr03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query--> Embeddings--->Relevant Context--->LLM--->Result"
      ],
      "metadata": {
        "id": "LlmZRZIZoyA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for level, questions in question_bank.items():\n",
        "    print(f\"\\n=== Difficulty: {level.upper()} ===\")\n",
        "    for user_question in questions:\n",
        "        query_vector = embed_model.encode([user_question])\n",
        "        D, I = index.search(query_vector, k=4)\n",
        "        relevant_chunks = [schema_chunks[i] for i in I[0]]\n",
        "        context = \"\\n\".join(relevant_chunks)\n",
        "\n",
        "        final_prompt = f\"\"\"\n",
        "[INST]You are a helpful SQL assistant. Based on the following schema and user question, write only a SQL query, please dont provide schema again.\n",
        "\n",
        "Schema:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{user_question}\n",
        "\n",
        "Return only the SQL query. [/INST]\n",
        "        \"\"\".strip()\n",
        "\n",
        "        response = pipe(final_prompt, max_new_tokens=250, do_sample=False)\n",
        "        generated_query = response[0][\"generated_text\"]\n",
        "\n",
        "        print(f\"\\nQuestion: {user_question}\")\n",
        "        print(\"Generated SQL:\\n\", generated_query)"
      ],
      "metadata": {
        "id": "sQVWTfKocr4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y286ZN50cr7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8yLBae1lcr-X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}